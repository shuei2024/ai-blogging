---
layout: post
title: "Detailed descriptions of AI Services usage for Image Generation task in AI Love project"
date: 2024-10-14 10:00:00 +0900
author: Tri Thanh
categories: AI Engineer, Researching
---

## Author: Tri Thanh


# Detailed descriptions of AI Services usage for Image Generation task in AI Love project


## Introduction

In the AI Love project, artificial intelligence plays a crucial role as the main component that interacts with users, ensuring their experience with the AI Love app is seamless and enjoyable. The AI system in this project is broken down into three interconnected parts, which collectively contribute to the final AI agents:

-   **Image Generation**: This function generates character images, allowing users to select their preferred character to start a conversation. Additionally, it produces various backgrounds or character variants depending on the conversation's context.
    
-   **Text-to-Speech**: This module converts the text responses generated by the characters (AI agents) into speech, ensuring that the voice matches the character’s context and persona.
    
-   **Facial Expression Generation**: This feature creates facial gestures, including movements of the face and mouth, based on the character’s image. The facial expressions are synchronized with the voice produced by the Text-to-Speech module.
    
This article aims to provide readers with ***detailed insights into the AI services employed for Image Generation tasks*** in the AI Love project. After thorough experimentation and discussions, we selected the following AI services:

- *Stable Diffusion 3 Large/Medium from Stability.AI* for generating character images.

- *RenderNet AI* for creating variants of the generated characters.

- *getimg.ai*, which is still under consideration for further experimentation via API.

## Stable Diffusion 3 Large/Medium từ Stability.AI

**Stable Diffusion 3** is an advanced generative model developed by Stability.AI for creating high-quality images from text prompts. It is a diffusion model, which means it generates images through a step-by-step process that gradually transforms random noise into coherent visuals. This method is based on a sequence of iterations, where the model refines the image progressively, conditioned on the input text.

The third version of Stable Diffusion, **Stable Diffusion 3**, represents a significant improvement over its predecessors in both image quality and control over the generation process. It uses a more efficient architecture that allows for higher resolution outputs and improved coherence between the generated image and the textual description. The model is particularly useful in applications requiring detailed, context-aware visuals, such as character design, art creation, and other image generation tasks where customization is important.

Some of the key features of **Stable Diffusion 3** include:

1.  **Higher Precision and Detail**: The model can generate intricate details in images, capturing subtle features such as textures, lighting effects, and fine structures.

2.  **Conditional Generation**: Stable Diffusion 3 excels in generating images that are closely aligned with the provided textual input, making it a powerful tool for creating customized visuals based on user specifications.

3.  **Scalability**: It supports both large and medium-scale models, providing flexibility in terms of resource usage, which is beneficial for applications with varying computational constraints.

4.  **Versatility**: The model can handle a wide range of styles and subjects, from realistic imagery to more abstract or artistic outputs, making it suitable for a variety of use cases.

Overall, **Stable Diffusion 3** represents a significant step forward in diffusion-based image generation, offering both technical advancements and practical applications across multiple domains, particularly where control over image creation is essential.

For the Image Generation requirements of the AI Love project, the generated characters must closely resemble real humans and possess a beautiful appearance. Additionally, several factors must be considered, such as cost, processing time, and integration capabilities within the system. After thorough consideration, we selected **Stable Diffusion 3 Large/Medium** for the following reasons:

1.  **High-quality human-like image generation** (see images below).

2.  **Cost-effective**: The pricing is reasonable, at $0.065 per image for Stable Diffusion 3 Large and $0.035 for Stable Diffusion 3 Medium. Additionally, Stable Diffusion 3 Medium allows for self-hosting after purchasing a full commercial license.

3.  **Comprehensive instructions** for using the Stability.AI API for the chosen models.

![Stable Diffusion 3 Large]({{ site.url }}/assets/Detailed_description_of_AI_services/SD3_Large.png)

![Stable Diffusion 3 Large Turbo]({{ site.url }}/assets/Detailed_description_of_AI_services/SD3_Large_Turbo.png)

![Stable Diffusion 3 Medium]({{ site.url }}/assets/Detailed_description_of_AI_services/SD3_Medium.png)

## RenderNet  AI

**RenderNet AI** is a state-of-the-art AI system designed for generating and modifying digital assets, specifically focusing on creating character variants, scenes, and other visual elements based on initial inputs. It is a powerful tool in creative and interactive applications where dynamic visual content is required, such as gaming, virtual reality, and AI-driven conversational agents.

RenderNet AI's key capabilities include:

1.  **Character Variant Generation**: RenderNet AI excels at creating different versions of a base character. This involves altering elements like clothing, hairstyles, expressions, and environments to match various contexts or preferences. These variants allow for greater personalization and adaptability in user interactions.
    
2.  **Contextual Customization**: The system can modify visual elements dynamically, aligning with the narrative or interaction taking place. For instance, a character's appearance may change depending on the storyline or the mood of the conversation, offering a rich and immersive experience for users.
    
3.  **Integration with Image Generation Models**: RenderNet AI is often used alongside models like Stable Diffusion to enhance the base image, allowing further refinement and variation. It serves as a post-processing tool that enriches initial designs with greater complexity and diversity.
    
4.  **Flexibility in Application**: The platform supports various artistic styles and use cases, ranging from realistic human avatars to more stylized, fictional characters. Its flexibility makes it suitable for industries like entertainment, education, and AI-based user interfaces.
    
In summary, **RenderNet AI** provides an advanced solution for generating diverse, contextually rich visual content. Its ability to create dynamic variants of characters and scenes from initial inputs makes it a critical tool for applications that require continuous adaptation and creativity, such as virtual interactions, AI storytelling, and immersive environments.

For the project, the characters must also adapt their backgrounds, settings, and outfits to suit the conversational context. Similar considerations of cost, processing time, and system integration were applied. After careful evaluation, we chose **RenderNet AI** for the following reasons:

1.  It allows the creation of a "character" from an input portrait and preserves the character’s history. Variants of the character retain similar qualities to the original portrait and are of high quality (see images below).

2.  **Cost-effective**: The pricing ranges from $7 to $79.

3.  **Comprehensive instructions** for using the RenderNet AI API, which is relatively easy to use. It can also be integrated with images generated by Stable Diffusion 3 to form a unified workflow.

4.  It offers various options and image transformations to suit different use cases.

![Original Image from SD3 Medium]({{ site.url }}/assets/Detailed_description_of_AI_services/Original_Image_from_SD3_Medium.png)

![Variant Image from RenderNet]({{ site.url }}/assets/Detailed_description_of_AI_services/Variant_Image_from_RenderNet.png)

## API Testing

Since the AI Love project will primarily use APIs to generate character images and create character variants based on context, coding and testing the APIs are essential steps before considering integration options within the Image Generator system or the final AI agents.

### Stable Diffusion 3 Large/Medium

First, I experimented with the Stable Diffusion 3 Large/Medium API following the instructions provided in the link: [SD3_API.ipynb - Colab (google.com)](https://colab.research.google.com/github/stability-ai/stability-sdk/blob/main/nbs/SD3_API.ipynb#scrollTo=0lDpGa2jAmAs). Through testing the three models (Stable Diffusion 3 Large, Large Turbo, and Medium), I observed that Stable Diffusion 3 Large/Medium generates the most realistic human-like images. Moreover, the API testing was straightforward, aided by the clear instructions provided in the link above.

### RenderNet AI

Once the character portraits were generated from Stable Diffusion 3, I tested RenderNet AI's API to verify its ability to generate variants for the characters created with Stable Diffusion 3. It seems that RenderNet AI is hosted on AWS, meaning the process of uploading images, creating characters, and generating variants all involve interactions with AWS-hosted services. To interact with RenderNet AI via API, I used the requests library to handle uploads and process data during testing.

The workflow for generating variants using RenderNet AI is as follows:

1. First, I uploaded the portrait generated by SD3 to RenderNet AI as an ***Asset***. This asset will then be used to create the character. I utilized the "Upload Asset V2" function to perform this step.

2. Second, I used the uploaded asset to create a ***Character***. The character included a name, description (prompt), and other relevant information. Here, I employed the "Create Character" function.

3. Third, I used the created Character to ***generate various variants*** according to the desired prompts and additional parameters such as the model to use, image size, etc. I utilized the "Generate Media" function to produce the variants.

A demo of the code used for this testing is provided in the following section [RenderNet_AI_API_test.ipynb](https://drive.google.com/file/d/13m_SjQWZ2dl9iQXZtCKpq7gMWVgl0-1y/view?usp=drive_link)

## References

[Stability AI Image Models — Stability AI](https://stability.ai/stable-image)
[RenderNet AI API](https://docs.rendernet.ai/api-reference/introduction)